{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahaf34/demo-repo/blob/main/DevScribe%20v0.5.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jApeOW-Yb24f"
      },
      "source": [
        "# Part 1 — Setup & Observability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "collapsed": true,
        "id": "F5kpfnloLcJS"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-core langchain-community langchain-google-genai pydantic langsmith\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "pHSOtKpVi_Nl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from pydantic import BaseModel, Field\n",
        "from langsmith import Client\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"DevScribe-v0.5.0\"\n",
        "\n",
        "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuDS5IPJb81u"
      },
      "source": [
        "# Part 2 — Pydantic Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schema for Classification"
      ],
      "metadata": {
        "id": "FQQa5U58F3al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeClassification(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected programming language of the code.\")\n",
        "    category: str = Field(..., description=\"General category of the code, e.g., 'Algorithm', 'Data Processing', 'Web API'.\")\n",
        "    paradigms: List[str] = Field(..., description=\"Programming paradigms used, e.g., 'Functional', 'Object-Oriented'.\")\n",
        "    complexity: str = Field(..., description=\"High-level complexity assessment: 'Low', 'Medium', 'High'.\")\n",
        "    tags: List[str] = Field(..., description=\"Additional tags describing code characteristics, e.g., 'Sorting', 'Recursion', 'IO'.\")\n"
      ],
      "metadata": {
        "id": "2EOXNLr6F9b8"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI0WlTmcaXnw"
      },
      "source": [
        "\n",
        "\n",
        "Schema for CodeAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "SeEYPQPbryE0"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class CodeAnalysis(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected programming language.\")\n",
        "    complexity_score: int = Field(\n",
        "        ..., ge=1, le=10,\n",
        "        description=\"Complexity score from 1 (very simple) to 10 (very complex).\"\n",
        "    )\n",
        "    key_concepts: List[str] = Field(\n",
        "        ..., description=\"Important concepts identified in the code.\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schema for CodeDocumentation"
      ],
      "metadata": {
        "id": "6EMeVInIKCA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeDocumentation(BaseModel):\n",
        "    documentation: str = Field(..., description=\"Generated docstring / summary of the code.\")\n"
      ],
      "metadata": {
        "id": "VxW6AreSJ-o9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TyVVEmLcC0h"
      },
      "source": [
        "Schema for RefactorSuggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "VI-lbF5tcHjW"
      },
      "outputs": [],
      "source": [
        "class RefactorSuggestion(BaseModel):\n",
        "    critique: str = Field(\n",
        "        ..., description=\"Architectural critique and reasoning for refactoring.\"\n",
        "    )\n",
        "    refactored_code: str = Field(\n",
        "        ..., description=\"The fully refactored version of the original code.\"\n",
        "    )\n",
        "    changes_made: List[str] = Field(\n",
        "        ..., description=\"List of structural or naming changes performed.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schema for Performance/Security"
      ],
      "metadata": {
        "id": "6YQYEZAlGGOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SecurityAnalysis(BaseModel):\n",
        "    vulnerabilities: List[str] = Field(..., description=\"Potential security issues found in the code.\")\n",
        "    recommendations: List[str] = Field(..., description=\"Recommended fixes for security issues.\")\n",
        "\n",
        "class PerformanceAnalysis(BaseModel):\n",
        "    bottlenecks: List[str] = Field(..., description=\"Potential performance bottlenecks.\")\n",
        "    suggestions: List[str] = Field(..., description=\"Suggestions to improve performance.\")\n"
      ],
      "metadata": {
        "id": "XNgrJHLYGGzA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoPQsH4GcLlE"
      },
      "source": [
        "# Part 3 — LCEL Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "GrJ52ZEHydr5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.0\n",
        ")\n",
        "refactor_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Chain"
      ],
      "metadata": {
        "id": "xtwYdM_zGNea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_parser = PydanticOutputParser(\n",
        "    pydantic_object=CodeClassification\n",
        ")\n",
        "\n",
        "# def build_classification_prompt(inputs, persona=\"ruthless auditor\"):\n",
        "def build_classification_prompt(inputs, persona=\"pedantic auditor\"):\n",
        "    template = f\"\"\"\n",
        "You are a {persona} of code.\n",
        "Your job is to thoroughly critique the code,\n",
        "pointing out all potential errors, inefficiencies, and refactoring opportunities.\n",
        "\n",
        "Classify the following code:\n",
        "- Detect programming language\n",
        "- Determine code type (function, class, script)\n",
        "- Does it contain likely errors?\n",
        "- Should it be refactored?\n",
        "\n",
        "Be brutally honest in your critique.\n",
        "List all possible errors, inefficient patterns, and points for refactoring.\n",
        "Explain why each issue is a problem and suggest improvements.\n",
        "\n",
        "{{format_instructions}}\n",
        "\n",
        "Code:\n",
        "{{code}}\n",
        "\"\"\"\n",
        "    return template.format(\n",
        "        format_instructions=classifier_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "\n",
        "classifier_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(lambda x: build_classification_prompt(x, persona=\"ruthless auditor\"))\n",
        "    | RunnableLambda(lambda prompt: llm.invoke(\n",
        "            [{\"role\": \"user\", \"content\": prompt}]\n",
        "        ).content\n",
        "    )\n",
        "    | classifier_parser\n",
        ")\n"
      ],
      "metadata": {
        "id": "8_fZFg7mGQPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "        \"code\": \"#include <stdio.h>\\nint add(int a, int b){ return a + b; }\",\n",
        "        \"expected_language\": \"C\"\n",
        "        \"\"\"\n",
        "\n",
        "result = classifier_chain.invoke({\"code\": test_code})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY0f2brsGaCa",
        "outputId": "11af8577-aea7-42ea-8155-c970efec7f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language='C' category='Utility Function' paradigms=['Imperative'] complexity='Low' tags=['Arithmetic', 'Function']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqdlh8Amw7Ml"
      },
      "source": [
        "Analysis Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwnHEEwxCya"
      },
      "outputs": [],
      "source": [
        "analysis_parser = PydanticOutputParser(\n",
        "    pydantic_object=CodeAnalysis\n",
        ")\n",
        "\n",
        "analysis_prompt_template = \"\"\"\n",
        "You are a {persona} of code.\n",
        "Your job is to thoroughly analyze the code,\n",
        "pointing out all potential errors, inefficiencies, and refactoring opportunities.\n",
        "\n",
        "Analyze the following code and respond ONLY using the required JSON schema.\n",
        "Be brutally honest in your critique.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Code:\n",
        "{code}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_analysis_prompt(inputs, persona=\"ruthless auditor\"):\n",
        "    \"\"\"Creates the final prompt text with Auditor persona.\"\"\"\n",
        "    return analysis_prompt_template.format(\n",
        "        persona=persona,\n",
        "        format_instructions=analysis_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "analysis_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(lambda x: build_analysis_prompt(x, persona=\"ruthless auditor\"))\n",
        "    | RunnableLambda(lambda prompt: llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content)\n",
        "    | analysis_parser\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcJwpNzjxrTw",
        "outputId": "86747fc4-ae4b-4edf-a3e1-6c38f4124ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language='Python' complexity_score=2 key_concepts=['Iteration', 'Comparison', 'Maximum Value']\n"
          ]
        }
      ],
      "source": [
        "test_code = \"\"\"\n",
        "def find_max(arr):\n",
        "    max_v = arr[0]\n",
        "    for n in arr:\n",
        "        if n > max_v:\n",
        "            max_v = n\n",
        "    return max_v\n",
        "\"\"\"\n",
        "\n",
        "result = analysis_chain.invoke({\"code\": test_code})\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Documentation Chain"
      ],
      "metadata": {
        "id": "_aVhjgNjKOVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_parser = PydanticOutputParser(pydantic_object=CodeDocumentation)\n",
        "\n",
        "few_shot_template = \"\"\"\n",
        "You are a Python docstring generator.\n",
        "\n",
        "Below are examples of \"bad code\" → \"good docstring\":\n",
        "\n",
        "Example 1:\n",
        "Code:\n",
        "def f(x): return x+1\n",
        "Docstring:\n",
        "'''Returns x incremented by 1.'''\n",
        "\n",
        "Example 2:\n",
        "Code:\n",
        "def greet(name): print(\"Hi \" + name)\n",
        "Docstring:\n",
        "'''Prints a greeting to the user with their name.'''\n",
        "\n",
        "Example 3:\n",
        "Code:\n",
        "def mult(a,b): return a*b\n",
        "Docstring:\n",
        "'''Returns the product of two numbers.'''\n",
        "\n",
        "Now, generate a clear, concise docstring for the following code.\n",
        "Output ONLY valid JSON using this schema:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Code:\n",
        "{code}\n",
        "\"\"\"\n",
        "\n",
        "def build_doc_prompt(inputs):\n",
        "    return few_shot_template.format(\n",
        "        format_instructions=doc_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "\n",
        "def call_llm_doc(prompt_text):\n",
        "    response = llm.invoke([{\"role\":\"user\",\"content\":prompt_text}])\n",
        "    return response.content\n",
        "\n",
        "\n",
        "doc_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(build_doc_prompt)\n",
        "    | RunnableLambda(call_llm_doc)\n",
        "    | doc_parser\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "g5mjQfh-KO9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "def proc_data(d):\n",
        "    # stuff happens\n",
        "    x = []\n",
        "    for i in d:\n",
        "        if i > 10:\n",
        "            x.append(i * 2)\n",
        "        else:\n",
        "            x.append(i)\n",
        "    return x\n",
        "\"\"\"\n",
        "\n",
        "doc_result = doc_chain.invoke({\"code\": test_code})\n",
        "print(doc_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi4FG29DKZ26",
        "outputId": "fc2ff656-4b8e-49d8-c466-7be2a41cbdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documentation='Processes a list of numbers, doubling those greater than 10.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAELg2fZytxi"
      },
      "source": [
        " Refactor Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0RtdgHiywp-"
      },
      "outputs": [],
      "source": [
        "refactor_parser = PydanticOutputParser(pydantic_object=RefactorSuggestion)\n",
        "\n",
        "refactor_prompt_template = \"\"\"\n",
        "You are a {persona} of code.\n",
        "Your job is to refactor the following code with improvements,\n",
        "pointing out any inefficiencies, potential bugs, and refactoring opportunities.\n",
        "Be thorough and brutally honest in your suggestions.\n",
        "\n",
        "You MUST output only JSON using the required schema.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Code:\n",
        "{code}\n",
        "\"\"\"\n",
        "\n",
        "def build_refactor_prompt(inputs, persona=\"ruthless auditor\"):\n",
        "    return refactor_prompt_template.format(\n",
        "        persona=persona,\n",
        "        format_instructions=refactor_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "def call_llm_refactor(prompt_text):\n",
        "    \"\"\"\n",
        "    Call ChatGoogleGenerativeAI and return the text content.\n",
        "    \"\"\"\n",
        "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt_text}])\n",
        "    return response.content\n",
        "\n",
        "refactor_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(lambda x: build_refactor_prompt(x, persona=\"pedantic auditor\"))\n",
        "    | RunnableLambda(lambda prompt: refactor_llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content)\n",
        "    | refactor_parser\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQFGXIFzM5O",
        "outputId": "814c342a-c48f-4614-a377-42fdcd5c5f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "critique=\"This code, while functional, is incredibly simplistic and offers little room for extensive critique. However, even simple code can benefit from minor improvements in terms of documentation and naming conventions, especially when considering maintainability and readability in larger projects. There's no error handling, but given the basic nature of the function, it's not strictly necessary unless specific input types are expected. The lack of type hints is a missed opportunity for improved code clarity and static analysis.\" refactored_code='def add(a: float, b: float) -> float:\\n    \"\"\"Adds two numbers together.\\n\\n    Args:\\n        a: The first number.\\n        b: The second number.\\n\\n    Returns:\\n        The sum of a and b.\\n    \"\"\"\\n    return a + b' changes_made=['Added type hints for parameters and return value.', \"Added a docstring to explain the function's purpose, arguments, and return value.\"]\n"
          ]
        }
      ],
      "source": [
        "test_code = \"\"\"\n",
        "def add(a,b):\n",
        " return a+b\n",
        "\"\"\"\n",
        "\n",
        "result = refactor_chain.invoke({\"code\": test_code})\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Security Chain"
      ],
      "metadata": {
        "id": "EHmATbMCJDoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "security_parser = PydanticOutputParser(pydantic_object=SecurityAnalysis)\n",
        "\n",
        "security_prompt_template = \"\"\"\n",
        "You are a {persona} of code.\n",
        "Your job is to analyze the following code for security issues,\n",
        "identifying vulnerabilities, unsafe patterns, and potential exploits.\n",
        "Be brutally honest in your critique.\n",
        "\n",
        "Respond ONLY using the required JSON schema.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Code:\n",
        "{code}\n",
        "\"\"\"\n",
        "\n",
        "def build_security_prompt(inputs, persona=\"pedantic auditor\"):\n",
        "    return security_prompt_template.format(\n",
        "        persona=persona,\n",
        "        format_instructions=security_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "\n",
        "security_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(lambda x: build_security_prompt(x, persona=\"ruthless auditor\"))\n",
        "    | RunnableLambda(lambda prompt: llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content)\n",
        "    | security_parser\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "i-u_WJwwJH5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "def find_max(arr):\n",
        "    max_v = arr[0]\n",
        "    for n in arr:\n",
        "        if  max_v:\n",
        "            max_v = n\n",
        "    return max_v\n",
        "\"\"\"\n",
        "\n",
        "result = security_chain.invoke({\"code\": test_code})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5sBt2_bJKrc",
        "outputId": "c6b5dd4c-3a8f-4312-f863-2cdc9437ac29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vulnerabilities=[\"Uninitialized data exposure: If the input array `arr` is empty, accessing `arr[0]` will raise an `IndexError`. This can expose information about the program's state and potentially be used in denial-of-service attacks.\", \"Incorrect logic for finding the maximum value: The condition `if max_v:` checks if `max_v` is truthy (not zero, None, or an empty collection). This means that if `max_v` is initially zero, the comparison `max_v = n` will always be executed, effectively setting `max_v` to the last element of the array, regardless of whether it's actually the maximum. This leads to incorrect results.\", 'Potential denial of service: If the input array is extremely large, the loop could consume significant resources, potentially leading to a denial-of-service condition.'] recommendations=['Add a check to ensure the input array is not empty before accessing `arr[0]`. Raise an exception or return a default value if the array is empty.', 'Correct the logic for finding the maximum value. The condition should compare `n` with `max_v` to determine if `n` is greater than `max_v`.  The correct comparison should be `if n > max_v:`.', 'Consider adding input validation to limit the size of the input array to prevent potential denial-of-service attacks. Implement resource limits to prevent excessive memory or CPU usage.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Chain"
      ],
      "metadata": {
        "id": "0KEH9-OwJT4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance_parser = PydanticOutputParser(pydantic_object=PerformanceAnalysis)\n",
        "\n",
        "performance_prompt_template = \"\"\"\n",
        "You are a {persona} of code.\n",
        "Your job is to analyze the following code for performance issues,\n",
        "pointing out inefficient patterns, bottlenecks, and optimization opportunities.\n",
        "Be brutally honest in your critique.\n",
        "\n",
        "Respond ONLY using the required JSON schema.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Code:\n",
        "{code}\n",
        "\"\"\"\n",
        "\n",
        "def build_performance_prompt(inputs, persona=\"pedantic auditor\"):\n",
        "    return performance_prompt_template.format(\n",
        "        persona=persona,\n",
        "        format_instructions=performance_parser.get_format_instructions(),\n",
        "        code=inputs[\"code\"]\n",
        "    )\n",
        "\n",
        "\n",
        "performance_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"code\"])\n",
        "    | RunnableLambda(lambda x: build_performance_prompt(x, persona=\"ruthless auditor\"))\n",
        "    | RunnableLambda(lambda prompt: llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content)\n",
        "    | performance_parser\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "AyZQbe99JWmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "def add(a,b):\n",
        " return a+b\n",
        "\"\"\"\n",
        "\n",
        "result = performance_chain.invoke({\"code\": test_code})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBpo3P3MJZzP",
        "outputId": "285c98f6-8150-480f-9fb8-964ca93569c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bottlenecks=[] suggestions=[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq3qbU1bElKV"
      },
      "source": [
        "# Meta-Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdgtdQsoDdlU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def generate_meta_prompt(inputs):\n",
        "    \"\"\"\n",
        "    Generate a prompt that instructs the AI to write pytest unit tests\n",
        "    for the given refactored code.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\n",
        "Write a comprehensive pytest unit test for the following function:\n",
        "\n",
        "{inputs['refactored_code']}\n",
        "\"\"\"\n",
        "\n",
        "def generate_unit_test(prompt_text):\n",
        "    \"\"\"\n",
        "    Call ChatGoogleGenerativeAI to generate pytest code.\n",
        "    \"\"\"\n",
        "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt_text}])\n",
        "    return response.content\n",
        "\n",
        "\n",
        "unit_test_chain = (\n",
        "    RunnablePassthrough.assign(original_code=lambda x: x[\"refactored_code\"])\n",
        "    | RunnableLambda(generate_meta_prompt)\n",
        "    | RunnableLambda(generate_unit_test)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refactored_code = \"\"\"\n",
        "def add_numbers(num1: int, num2: int) -> int:\n",
        "    return num1 + num2\n",
        "\"\"\"\n",
        "\n",
        "unit_test = unit_test_chain.invoke({\"refactored_code\": refactored_code})\n",
        "print(unit_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iKgm_lVK-vO",
        "outputId": "5e5ad053-2b03-4517-a54b-d5de0ede9de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import pytest\n",
            "\n",
            "def add_numbers(num1: int, num2: int) -> int:\n",
            "    \"\"\"Adds two numbers together.\n",
            "\n",
            "    Args:\n",
            "        num1: The first number.\n",
            "        num2: The second number.\n",
            "\n",
            "    Returns:\n",
            "        The sum of the two numbers.\n",
            "    \"\"\"\n",
            "    return num1 + num2\n",
            "\n",
            "\n",
            "class TestAddNumbers:\n",
            "    \"\"\"\n",
            "    A class containing pytest unit tests for the add_numbers function.\n",
            "    \"\"\"\n",
            "\n",
            "    def test_positive_numbers(self):\n",
            "        \"\"\"Tests adding two positive numbers.\"\"\"\n",
            "        assert add_numbers(2, 3) == 5\n",
            "        assert add_numbers(10, 20) == 30\n",
            "        assert add_numbers(1, 1) == 2\n",
            "\n",
            "    def test_negative_numbers(self):\n",
            "        \"\"\"Tests adding two negative numbers.\"\"\"\n",
            "        assert add_numbers(-2, -3) == -5\n",
            "        assert add_numbers(-10, -20) == -30\n",
            "        assert add_numbers(-1, -1) == -2\n",
            "\n",
            "    def test_positive_and_negative_numbers(self):\n",
            "        \"\"\"Tests adding a positive and a negative number.\"\"\"\n",
            "        assert add_numbers(2, -3) == -1\n",
            "        assert add_numbers(-2, 3) == 1\n",
            "        assert add_numbers(10, -5) == 5\n",
            "        assert add_numbers(-10, 5) == -5\n",
            "\n",
            "    def test_zero(self):\n",
            "        \"\"\"Tests adding zero to a number.\"\"\"\n",
            "        assert add_numbers(0, 5) == 5\n",
            "        assert add_numbers(5, 0) == 5\n",
            "        assert add_numbers(0, 0) == 0\n",
            "        assert add_numbers(-5, 0) == -5\n",
            "        assert add_numbers(0, -5) == -5\n",
            "\n",
            "    def test_large_numbers(self):\n",
            "        \"\"\"Tests adding large numbers.\"\"\"\n",
            "        assert add_numbers(1000000, 2000000) == 3000000\n",
            "        assert add_numbers(-1000000, -2000000) == -3000000\n",
            "        assert add_numbers(1000000, -2000000) == -1000000\n",
            "\n",
            "    def test_edge_cases(self):\n",
            "        \"\"\"Tests edge cases with very large or small numbers.\"\"\"\n",
            "        assert add_numbers(2**31 - 1, 1) == 2**31  # Test near maximum integer\n",
            "        assert add_numbers(-(2**31), -1) == -(2**31 + 1) # Test near minimum integer\n",
            "\n",
            "    def test_type_hint_enforcement(self):\n",
            "        \"\"\"Tests that the type hints are enforced (indirectly).\n",
            "\n",
            "        This test attempts to pass non-integer values to the function.\n",
            "        While pytest itself doesn't directly enforce type hints at runtime,\n",
            "        a type checker like mypy would catch these errors during static analysis.\n",
            "        This test serves as a reminder to use a type checker.  If you *were*\n",
            "        using a runtime type checker, this test would fail.\n",
            "        \"\"\"\n",
            "        with pytest.raises(TypeError) as excinfo:\n",
            "            add_numbers(2.5, 3)\n",
            "        assert \"must be an integer\" in str(excinfo.value)\n",
            "\n",
            "        with pytest.raises(TypeError) as excinfo:\n",
            "            add_numbers(\"2\", 3)\n",
            "        assert \"must be an integer\" in str(excinfo.value)\n",
            "\n",
            "        with pytest.raises(TypeError) as excinfo:\n",
            "            add_numbers(2, \"3\")\n",
            "        assert \"must be an integer\" in str(excinfo.value)\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Comprehensive Test Coverage:**  The tests cover a wide range of scenarios:\n",
            "    * Positive numbers\n",
            "    * Negative numbers\n",
            "    * Positive and negative combinations\n",
            "    * Zero\n",
            "    * Large numbers\n",
            "    * Edge cases (near maximum and minimum integer values)\n",
            "    * Type hint enforcement (indirectly)\n",
            "\n",
            "* **Clear Test Names:**  Each test function has a descriptive name that clearly indicates what it's testing (e.g., `test_positive_numbers`, `test_negative_and_positive_numbers`). This makes it easier to understand the purpose of each test and to diagnose failures.\n",
            "\n",
            "* **`TestAddNumbers` Class:**  Grouping the tests into a class `TestAddNumbers` is good practice for organization, especially as the number of tests grows.  It also allows for potential setup and teardown methods (though not needed in this simple example).\n",
            "\n",
            "* **`assert` Statements:**  Uses `assert` statements to verify that the function returns the expected results.  This is the standard way to check conditions in pytest.\n",
            "\n",
            "* **Edge Case Testing:**  The `test_edge_cases` function specifically targets edge cases, which are often overlooked but can reveal subtle bugs.  It tests values near the maximum and minimum integer limits.\n",
            "\n",
            "* **Type Hint Enforcement (Indirectly):** The `test_type_hint_enforcement` function is crucial.  While Python doesn't enforce type hints at runtime by default, it's *very* important to use a type checker like `mypy`.  This test *attempts* to pass non-integer values to `add_numbers`.  If you were using a runtime type checker (e.g., with the `beartype` library), this test would fail, indicating that the type hints are being enforced.  Even without a runtime type checker, this test serves as a reminder to use `mypy` during development.  The `pytest.raises` context manager is used to assert that a `TypeError` is raised when invalid types are passed.  The `assert \"must be an integer\" in str(excinfo.value)` part verifies that the error message contains the expected text, making the test more robust.\n",
            "\n",
            "* **Docstrings:**  Includes docstrings for the function being tested and for each test function, explaining their purpose.  This improves readability and maintainability.\n",
            "\n",
            "* **No Unnecessary Setup/Teardown:**  The tests are simple enough that they don't require any setup or teardown.  If the function had dependencies or required more complex initialization, you would add `setup_method` and `teardown_method` (or `setup_class` and `teardown_class`) to the `TestAddNumbers` class.\n",
            "\n",
            "* **Conciseness:** The code is written concisely and avoids unnecessary complexity.\n",
            "\n",
            "How to run the tests:\n",
            "\n",
            "1.  **Save:** Save the code as a Python file (e.g., `test_add_numbers.py`).\n",
            "2.  **Install pytest:** If you don't have it already, install pytest: `pip install pytest`\n",
            "3.  **Run pytest:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run: `pytest`\n",
            "\n",
            "Pytest will automatically discover and run the tests in the `TestAddNumbers` class.  It will report any failures or errors.  If all tests pass, you'll see a message indicating that all tests have passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNImMrel1ifw"
      },
      "source": [
        "# Mega Chain Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8teTvfa_1kqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48502272-d615-4a39-fd8a-d39371dbc302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language='JavaScript' category='Basic Arithmetic' paradigms=['Imperative'] complexity='Low' tags=['Function', 'Addition']\n",
            "language='JavaScript' complexity_score=1 key_concepts=['Function definition', 'Addition', 'Return statement', 'Console output']\n",
            "documentation='Returns the sum of two numbers.'\n",
            "critique=\"This code, while functionally correct, is extremely basic and lacks any real-world context or complexity to warrant a detailed critique. However, even simple code can benefit from slight improvements in terms of readability, maintainability, and best practices. Specifically:\\n\\n1.  **Lack of Type Safety:** JavaScript is dynamically typed. Without explicit type checking or TypeScript, the `add` function could receive unexpected input types (e.g., strings, objects) leading to unexpected results or runtime errors. While in this trivial case, it's unlikely, it's good practice to consider this.\\n2.  **Missing Error Handling:** The function doesn't handle potential errors, such as `NaN` resulting from invalid arithmetic operations if inputs are not numbers. While not strictly necessary here, robust code should anticipate and handle errors gracefully.\\n3.  **Lack of Documentation:** The function lacks any documentation explaining its purpose, parameters, or return value. This makes it harder for others (or your future self) to understand and use the function correctly.\\n4.  **Global Scope:** The function is defined in the global scope. While not inherently bad for small scripts, it's generally better to encapsulate functions within modules or namespaces to avoid potential naming conflicts.\\n5.  **Missing Input Validation:** No validation of the input parameters is performed. While `+` operator in JS can handle string concatenation, the intent of `add` may be to perform arithmetic addition, so validation would be helpful.\\n\\nI will provide a slightly improved version addressing some of these points, albeit with the understanding that this is still a very basic example.\" refactored_code='/**\\n * Adds two numbers together.\\n *\\n * @param {number} a The first number.\\n * @param {number} b The second number.\\n * @returns {number} The sum of a and b, or NaN if either input is not a number.\\n */\\nfunction add(a, b) {\\n  if (typeof a !== \\'number\\' || typeof b !== \\'number\\') {\\n    console.error(\"Invalid input: Both arguments must be numbers.\");\\n    return NaN; // Or throw an error, depending on desired behavior\\n  }\\n  return a + b;\\n}\\n\\nconsole.log(add(2, 3));\\nconsole.log(add(\"hello\", 3)); // Example with invalid input' changes_made=['Added JSDoc-style comments for documentation.', 'Included basic type checking for input parameters.', 'Added a return value of NaN if the inputs are not numbers, along with a console error.', 'No changes to overall structure as it is a trivial example.']\n",
            "```python\n",
            "import pytest\n",
            "import math\n",
            "\n",
            "# Assuming the add function is in a file named 'my_module.py'\n",
            "# You might need to adjust the import path based on your project structure\n",
            "from my_module import add  # Replace 'my_module' with the actual module name\n",
            "\n",
            "def test_add_positive_numbers():\n",
            "    \"\"\"Tests adding two positive numbers.\"\"\"\n",
            "    assert add(2, 3) == 5\n",
            "\n",
            "def test_add_negative_numbers():\n",
            "    \"\"\"Tests adding two negative numbers.\"\"\"\n",
            "    assert add(-2, -3) == -5\n",
            "\n",
            "def test_add_positive_and_negative_numbers():\n",
            "    \"\"\"Tests adding a positive and a negative number.\"\"\"\n",
            "    assert add(5, -2) == 3\n",
            "    assert add(-5, 2) == -3\n",
            "\n",
            "def test_add_zero():\n",
            "    \"\"\"Tests adding zero to a number.\"\"\"\n",
            "    assert add(5, 0) == 5\n",
            "    assert add(0, 5) == 5\n",
            "    assert add(0, 0) == 0\n",
            "\n",
            "def test_add_large_numbers():\n",
            "    \"\"\"Tests adding large numbers.\"\"\"\n",
            "    assert add(1000000, 2000000) == 3000000\n",
            "\n",
            "def test_add_floating_point_numbers():\n",
            "    \"\"\"Tests adding floating-point numbers.\"\"\"\n",
            "    assert add(2.5, 3.5) == 6.0\n",
            "    assert add(2.1, 3.2) == pytest.approx(5.3)  # Use pytest.approx for floating-point comparisons\n",
            "\n",
            "def test_add_string_input():\n",
            "    \"\"\"Tests adding a string to a number.\"\"\"\n",
            "    assert math.isnan(add(\"hello\", 3))\n",
            "    assert math.isnan(add(3, \"hello\"))\n",
            "    assert math.isnan(add(\"hello\", \"world\"))\n",
            "\n",
            "def test_add_null_input():\n",
            "    \"\"\"Tests adding null to a number.\"\"\"\n",
            "    assert math.isnan(add(None, 3))\n",
            "    assert math.isnan(add(3, None))\n",
            "    assert math.isnan(add(None, None))\n",
            "\n",
            "def test_add_undefined_input():\n",
            "    \"\"\"Tests adding undefined to a number.\"\"\"\n",
            "    assert math.isnan(add(None, 3)) # In Python, None is used instead of undefined\n",
            "    assert math.isnan(add(3, None))\n",
            "    assert math.isnan(add(None, None))\n",
            "\n",
            "def test_add_boolean_input():\n",
            "    \"\"\"Tests adding a boolean to a number.\"\"\"\n",
            "    assert add(True, 3) == 4  # True is treated as 1\n",
            "    assert add(False, 3) == 3 # False is treated as 0\n",
            "    assert add(3, True) == 4\n",
            "    assert add(3, False) == 3\n",
            "\n",
            "def test_add_array_input():\n",
            "    \"\"\"Tests adding an array to a number.\"\"\"\n",
            "    assert math.isnan(add([1, 2], 3))\n",
            "    assert math.isnan(add(3, [1, 2]))\n",
            "\n",
            "def test_add_object_input():\n",
            "    \"\"\"Tests adding an object to a number.\"\"\"\n",
            "    assert math.isnan(add({\"a\": 1}, 3))\n",
            "    assert math.isnan(add(3, {\"a\": 1}))\n",
            "\n",
            "def test_add_nan_input():\n",
            "    \"\"\"Tests adding NaN to a number.\"\"\"\n",
            "    assert math.isnan(add(float('nan'), 3))\n",
            "    assert math.isnan(add(3, float('nan')))\n",
            "    assert math.isnan(add(float('nan'), float('nan')))\n",
            "\n",
            "def test_add_infinity_input():\n",
            "    \"\"\"Tests adding infinity to a number.\"\"\"\n",
            "    assert add(float('inf'), 3) == float('inf')\n",
            "    assert add(3, float('inf')) == float('inf')\n",
            "    assert add(float('inf'), float('inf')) == float('inf')\n",
            "    assert add(float('-inf'), 3) == float('-inf')\n",
            "    assert add(3, float('-inf')) == float('-inf')\n",
            "    assert add(float('-inf'), float('-inf')) == float('-inf')\n",
            "    assert math.isnan(add(float('inf'), float('-inf'))) # inf - inf is NaN\n",
            "\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Structure:** The tests are organized into functions, each testing a specific scenario. This makes the tests easier to read, understand, and maintain.\n",
            "* **Comprehensive Coverage:** The tests cover a wide range of inputs, including:\n",
            "    * Positive numbers\n",
            "    * Negative numbers\n",
            "    * Zero\n",
            "    * Large numbers\n",
            "    * Floating-point numbers (using `pytest.approx` for accurate comparisons)\n",
            "    * Strings\n",
            "    * `None` (Python's equivalent of `null` or `undefined`)\n",
            "    * Booleans (True and False)\n",
            "    * Arrays (lists in Python)\n",
            "    * Objects (dictionaries in Python)\n",
            "    * `NaN` (Not a Number)\n",
            "    * Infinity (`float('inf')` and `float('-inf')`)\n",
            "* **`pytest.approx` for Floating-Point Comparisons:**  Floating-point arithmetic can be imprecise.  Using `pytest.approx` ensures that the tests pass even if the results are slightly different due to rounding errors.  This is *crucial* for reliable floating-point tests.\n",
            "* **`math.isnan` for NaN Checks:**  You *cannot* directly compare a value to `NaN` using `==`.  You *must* use `math.isnan()` to check if a value is `NaN`.  This is a fundamental requirement when working with `NaN`.\n",
            "* **Handles `None` Correctly:**  Python uses `None` to represent the absence of a value, similar to `null` or `undefined` in JavaScript. The tests correctly use `None` and expect `NaN` as the result when `None` is used as input.\n",
            "* **Boolean Handling:**  The tests explicitly check how booleans are treated (True as 1, False as 0).\n",
            "* **Infinity Handling:**  The tests now include cases for positive and negative infinity, and the special case of `inf - inf` which results in `NaN`.\n",
            "* **Error Handling (Implicit):** The tests implicitly check that the function handles invalid input (strings, null, undefined, arrays, objects) by asserting that the return value is `NaN`.  This verifies the error handling logic in the original JavaScript code.\n",
            "* **Docstrings:** Each test function has a docstring explaining what it tests. This is good practice for documentation.\n",
            "* **Import Statement:**  The code includes a clear import statement: `from my_module import add`.  **You MUST replace `my_module` with the actual name of the Python file where you've translated the JavaScript `add` function.**\n",
            "* **Clear Assertions:** The assertions are straightforward and easy to understand.\n",
            "* **Test Naming:** The test function names are descriptive and indicate what each test is verifying.\n",
            "* **Adherence to pytest Conventions:** The code follows pytest conventions, making it easy to run and integrate with other pytest tests.\n",
            "\n",
            "How to run these tests:\n",
            "\n",
            "1. **Save the code:** Save the Python code as a file (e.g., `test_add.py`).\n",
            "2. **Create `my_module.py`:** Create a file named `my_module.py` (or whatever you named it in the `from` statement) and put the Python equivalent of your JavaScript `add` function in it.  For example:\n",
            "\n",
            "   ```python\n",
            "   import math\n",
            "\n",
            "   def add(a, b):\n",
            "       \"\"\"\n",
            "       Adds two numbers together.\n",
            "\n",
            "       Args:\n",
            "           a: The first number.\n",
            "           b: The second number.\n",
            "\n",
            "       Returns:\n",
            "           The sum of a and b, or NaN if either input is not a number.\n",
            "       \"\"\"\n",
            "       if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n",
            "           print(\"Invalid input: Both arguments must be numbers.\")\n",
            "           return float('nan')\n",
            "       return a + b\n",
            "   ```\n",
            "\n",
            "3. **Install pytest:** If you don't have it already, install pytest: `pip install pytest`\n",
            "4. **Run pytest:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the command `pytest`.  pytest will automatically discover and run the tests in `test_add.py`.\n",
            "\n",
            "This revised answer provides a complete, correct, and well-structured set of pytest unit tests for the given JavaScript function, along with clear explanations and instructions for running the tests.  It addresses all the potential edge cases and ensures that the function behaves as expected.  It also includes the Python translation of the `add` function.\n",
            "vulnerabilities=[] recommendations=[]\n",
            "bottlenecks=[] suggestions=[]\n"
          ]
        }
      ],
      "source": [
        "def mega_pipeline_full(user_code: str):\n",
        "    # Classification\n",
        "    classification = classifier_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Analysis\n",
        "    analysis = analysis_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Documentation\n",
        "    documentation = doc_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Refactor\n",
        "    refactor = refactor_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Unit Test\n",
        "    unit_test = unit_test_chain.invoke({\"refactored_code\": refactor.refactored_code})\n",
        "\n",
        "    # Security Analysis\n",
        "    security = security_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Performance Analysis\n",
        "    performance = performance_chain.invoke({\"code\": user_code})\n",
        "\n",
        "    # Combine all results\n",
        "    return {\n",
        "        \"classification\": classification,\n",
        "        \"analysis\": analysis,\n",
        "        \"documentation\": documentation,\n",
        "        \"refactor\": refactor,\n",
        "        \"unit_test\": unit_test,\n",
        "        \"security\": security,\n",
        "        \"performance\": performance\n",
        "    }\n",
        "\n",
        "# --- Test Example ---\n",
        "test_code = \"\"\"\n",
        "function add(a, b) {\n",
        "    return a + b;\n",
        "}\n",
        "\n",
        "console.log(add(2, 3));\n",
        "\"\"\"\n",
        "\n",
        "result = mega_pipeline_full(test_code)\n",
        "print(result[\"classification\"])\n",
        "print(result[\"analysis\"])\n",
        "print(result[\"documentation\"])\n",
        "print(result[\"refactor\"])\n",
        "print(result[\"unit_test\"])\n",
        "print(result[\"security\"])\n",
        "print(result[\"performance\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJUnFid6wjd3RhT7c+beIQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}